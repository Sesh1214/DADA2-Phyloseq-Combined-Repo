#Before starting, make sure to unzip the fastq files. Also do an interactive session and load R/4.2.1. Can use gzip -d command on terminal to do this
library(dada2)
path <- "/gpfs/afm/cancergenetics/Seshadhri/Stephanie_Schuller_Data/Fastq"
#The list files command should list all the paired end reads for each sample.
list.files(path)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

#Plot the Read Quality for the forward and reverse paired end reads and put them in separate PDFs
#This part of the code does not work properly in HPC. Have to figure out how to get these plots into a PDF and put that in. Possibly assign each of the plots to variables and then use ggplot? The other option is to do the entire pipeline locally simply because the datasets are going to be small and then go on to upload the results onto the shared database. But it would be useful to actually know how to get everything onto HPC. Although saying that, even for my own data I will always need to look at the Quality profile before deciding filter criteria so maybe that should be done locally?
#pdf(file = "Read_Quality_Profiles_FWD.pdf", width = 4, height = 4)
#plotQualityProfile(fnFs[1:6])
#plotQualityProfile(fnFs[7:12])
#plotQualityProfile(fnFs[13:18])
#plotQualityProfile(fnFs[19:24])
#plotQualityProfile(fnFs[25:30])
#dev.off()
#pdf(file = "Read_Quality_Profiles_REV.pdf", width = 4, height = 4)
#plotQualityProfile(fnRs[1:6])
#plotQualityProfile(fnRs[7:12])
#plotQualityProfile(fnRs[13:18])
#plotQualityProfile(fnRs[19:24])
#plotQualityProfile(fnRs[25:30])
#dev.off()

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(230,240), maxN=0, maxEE=c(10,10), minQ=2, truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=FALSE)
              
print(out)

errF <- learnErrors(filtFs, multithread=FALSE)
errR <- learnErrors(filtRs, multithread=FALSE)

#pdf(file = "Error_Plots.pdf", width = 4, height = 4)
#plotErrors(errF, nominalQ=TRUE)
#plotErrors(errR, nominalQ=TRUE)
#dev.off()

dadaFs <- dada(filtFs, err=errF, multithread=FALSE)
dadaRs <- dada(filtRs, err=errR, multithread=FALSE)
dadaFs[[1]]

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

seqtab <- makeSequenceTable(mergers)
dim(seqtab)

table(nchar(getSequences(seqtab)))

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=FALSE, verbose=TRUE)
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)

#Track Number of Reads through the pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

db_path = "/gpfs/afm/cancergenetics/Seshadhri/Stephanie_Schuller_Data/GTDB_bac-arc_ssu_r86.fa.gz"

taxa <- assignTaxonomy(seqtab.nochim, db_path, multithread=FALSE)
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
write.csv("Taxa_Print.csv", taxa.print)
write.csv("Seqtab_Nochim.csv", seqtab.nochim)
